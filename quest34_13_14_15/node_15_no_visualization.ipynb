{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533444fe",
   "metadata": {},
   "source": [
    "## 프로젝트: 더 멋진 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b9ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "print(tf.__version__)\n",
    "print(numpy.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb93fb1",
   "metadata": {},
   "source": [
    "### 데이터 다운로드 (클라우드 유저용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefd544",
   "metadata": {},
   "source": [
    "### 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f363d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#병렬데이터 중복 제거\n",
    "def duplication_remover(enc_corpus, dec_corpus ):\n",
    "    cleaned_corpus = set(zip(enc_corpus, dec_corpus))\n",
    "    see = list(cleaned_corpus)\n",
    "    see[:5]\n",
    "    return list(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64eeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = \"./data/korean-english-park.train.ko\"\n",
    "eng_path = \"./data/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "    enc_corpus = kor\n",
    "    dec_corpus = eng\n",
    "    cleaned_corpus = duplication_remover(enc_corpus, dec_corpus )\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1d26e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('이에 3십6만 명이 넘는 외국인 노동자들이 노동 착취 및 작업장에서의 불공정한 대우를 받지 않고 지금보다 더 나은 조건에서 일을 할 수 있게 됐다.2005년 5월 서울 경기-인천 이주노동자 노동조합(MTU)이 서울지방노동청장을 상대로 노조설립신고서 반려 처분 취소 소송을 냈었다.',\n",
       " 'In May 2005, the Seoul-Gyeonggi-Incheon Migrant Trade Union submitted an application to launch a legal labor union at a Seoul regional office of the Labor Ministry.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8912f8",
   "metadata": {},
   "source": [
    "> * 모든 입력을 소문자로 변환합니다.   -> ok\n",
    "> * 알파벳, 문장부호, 한글만 남기고 모두 제거합니다. ->ok\n",
    "> * 문장부호 양옆에 공백을 추가합니다.  -> ok\n",
    "> * 문장 앞뒤의 불필요한 공백을 제거합니다.->ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d7c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For korean\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_sentence_kor(sentence): \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # Add spaces around punctuation\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "\n",
    "    # Remove consecutive spaces\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # Remove all characters except Korean, spaces, and punctuation\n",
    "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    \n",
    "    # Remove leading/trailing spaces\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    #if s_token:\n",
    "    #    sentence = '<start> ' + sentence\n",
    "\n",
    "    #if e_token:\n",
    "    #    sentence += ' <end>'\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2912ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For English\n",
    "\n",
    "def preprocess_sentence_en(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    # Replace to lower characters\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fe2a1",
   "metadata": {},
   "source": [
    "### Sentencepiece를 활용하여 학습한 tokenizer를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b904c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "def sentencepiece_learning(corpus, vocab_size, lang, pad_id, bos_id, eos_id, unk_id):\n",
    "\n",
    "    temp_file = './data/korean-english-park.train.ko.temp'\n",
    "    #temp_file = os.getenv('HOME')+'/aiffel/workspace/goingdeeper/sp_tokenizer/data/korean_spm_bpe_10k.vocab'\n",
    "\n",
    "\n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "            f.write(str(row) + '\\n')\n",
    "\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=temp_file,\n",
    "        model_prefix=f\"{lang}\",\n",
    "        vocab_size=vocab_size,\n",
    "        pad_id=pad_id,\n",
    "        bos_id=bos_id,\n",
    "        eos_id=eos_id,\n",
    "        unk_id=unk_id\n",
    "    )\n",
    "    \n",
    "    #return korean_spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8cc5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sp_vocab(lang):\n",
    "    \n",
    "    with open('./'+ lang +'.vocab', 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "        \n",
    "    return word_index, index_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513f2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(tokenizer, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tokensIDs = tokenizer.EncodeAsIds(sen)\n",
    "        tensor.append(tokenizer.EncodeAsIds(sen))\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')\n",
    "    \n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc3ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    # [[YOUR CODE]]\n",
    "    # voca, model 생성\n",
    "    sentencepiece_learning(corpus, vocab_size, lang, pad_id, bos_id, eos_id, unk_id)\n",
    "    \n",
    "    # korean_spm.voca 읽어오기 \n",
    "    word_index, index_word = read_sp_vocab(lang)\n",
    "    \n",
    "    #Sentence Piece instance 만들고 korean_spm.model load하기\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f\"{lang}.model\")\n",
    "    \n",
    "    tokenizer = sp_tokenize(tokenizer, corpus)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a01b4",
   "metadata": {},
   "source": [
    "### 전처리 함수, Tokenize 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13073d36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: ko\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ./data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78967 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5192552\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1217\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 162645 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 211395\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 211395 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=85800 obj=12.9963 num_tokens=418836 num_tokens/piece=4.88154\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=72989 obj=11.8412 num_tokens=420481 num_tokens/piece=5.76088\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=54735 obj=11.8505 num_tokens=438075 num_tokens/piece=8.00356\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=54712 obj=11.8166 num_tokens=438537 num_tokens/piece=8.01537\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=41034 obj=11.9628 num_tokens=463298 num_tokens/piece=11.2906\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=41034 obj=11.926 num_tokens=463342 num_tokens/piece=11.2917\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=30775 obj=12.1221 num_tokens=490146 num_tokens/piece=15.9268\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=30775 obj=12.0803 num_tokens=490142 num_tokens/piece=15.9266\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=23081 obj=12.3249 num_tokens=517931 num_tokens/piece=22.4397\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=23081 obj=12.278 num_tokens=518024 num_tokens/piece=22.4437\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=12.3213 num_tokens=522296 num_tokens/piece=23.7407\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=12.313 num_tokens=522296 num_tokens/piece=23.7407\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ko.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ko.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ./data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=11767037\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9918% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=31\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999917\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78968 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 79171 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78968\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 44564\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 44564 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=33325 obj=11.1908 num_tokens=84851 num_tokens/piece=2.54617\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25249 obj=8.85935 num_tokens=85851 num_tokens/piece=3.40017\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21292 obj=8.77457 num_tokens=86563 num_tokens/piece=4.06552\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21103 obj=8.75326 num_tokens=87067 num_tokens/piece=4.12581\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for k, e in cleaned_corpus:\n",
    "    #k, e = pair.split(\"\\t\")\n",
    "    kor_corpus.append(preprocess_sentence_kor(k))\n",
    "    eng_corpus.append(preprocess_sentence_en(e, s_token=True, e_token=True))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1caa1d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ko_tokenizer), len(en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76e2d8",
   "metadata": {},
   "source": [
    "### 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "131d2216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9c9754663d4cfbb32c7e4d45520c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# Keep only sentences with token length less than or equal to 50\n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    kor_tokenized = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    eng_tokenized = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    if len(kor_tokenized) <= 50 and len(eng_tokenized) <= 50:\n",
    "        src_corpus.append(kor_tokenized)\n",
    "        tgt_corpus.append(eng_tokenized)\n",
    "\n",
    "# Pad the sequences to create training data\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9dd6f1",
   "metadata": {},
   "source": [
    "### 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5848f4",
   "metadata": {},
   "source": [
    "A transformer is a type of neural network architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. (2017). It is commonly used for natural language processing tasks such as machine translation, text generation, and text classification.\n",
    "\n",
    "To implement a transformer, you will need several functions and classes to define the different components of the architecture. Here are some of the key components you will need:\n",
    "\n",
    "1. **Multi-head attention**: This is a core component of the transformer architecture that allows the model to weigh the importance of different parts of the input sequence when making predictions. You will need to implement a function or class to compute the scaled dot-product attention and combine multiple attention heads.\n",
    "\n",
    "2. **Feed-forward neural network**: The transformer architecture includes a feed-forward neural network in each layer, applied to each position separately and identically. You will need to implement a function or class to define this feed-forward neural network.\n",
    "\n",
    "3. **Layer normalization**: The transformer architecture uses layer normalization to stabilize the training process. You will need to implement a function or class to perform layer normalization.\n",
    "\n",
    "4. **Positional encoding**: The transformer architecture does not have any inherent notion of position, so it relies on positional encoding to incorporate information about the relative positions of the elements in the input sequence. You will need to implement a function or class to compute the positional encoding.\n",
    "\n",
    "5. **Encoder and decoder**: The transformer architecture consists of an encoder and a decoder, each composed of multiple layers of multi-head attention and feed-forward neural networks. You will need to implement classes to define the encoder and decoder.\n",
    "\n",
    "6. **Masking**: The transformer architecture uses masking to prevent the model from attending to certain positions in the input sequence. You will need to implement functions to generate the appropriate masks for the encoder and decoder.\n",
    "\n",
    "Once you have implemented these components, you can combine them to create a complete transformer model. There are many resources available online that provide detailed explanations and examples of how to implement a transformer, so you may find it helpful to refer to these resources as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381f323",
   "metadata": {},
   "source": [
    "### 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ac670",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8a3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d960664",
   "metadata": {},
   "source": [
    "### Multi-head attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf04ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "      \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6562c1",
   "metadata": {},
   "source": [
    "### Feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "206a12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2948f",
   "metadata": {},
   "source": [
    "### Encoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c066a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d142d8",
   "metadata": {},
   "source": [
    "### Decoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02f3f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bea39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2de2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96ca08",
   "metadata": {},
   "source": [
    "### Transformer 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea6aee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # Step 1: Embedding\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        # Step 2: Encoder\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        # Step 3: Decoder\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "            self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        # Step 4: Out Linear\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91d4cd",
   "metadata": {},
   "source": [
    "### Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b6652d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fcb1f7",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f7425bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ecb2a",
   "metadata": {},
   "source": [
    "### Attention 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17a425cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3d2b8",
   "metadata": {},
   "source": [
    "### 번역 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d6f4ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "#그래프 한글표현 인스톨?\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "211fa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence_kor(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, \n",
    "                                                                 combined_mask, dec_padding_mask)\n",
    "\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da7de3",
   "metadata": {},
   "source": [
    "### 번역 생성 및 Attention 시각화 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88b03283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4b14d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#masking\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46763885",
   "metadata": {},
   "source": [
    "### Train Step 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c94d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "    # Compute the loss using tf.GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # Compute the gradients and update the model weights\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d71c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58926, 58926)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_train), len(dec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efc685f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm.tqdm_notebook #tqdm.notebook.tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm_notebook \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_evaluate(epoch, enc_train, dec_train, optimizer, ko_tokenizer, en_tokenizer, src_vocab_size, tgt_vocab_size):\n",
    "    \n",
    "    print('epoch :', epoch)\n",
    "    print('batch_size :', batch_size)\n",
    "    print('vocab_size :', vocab_size )\n",
    "    \n",
    "    examples = [\n",
    "                \"오바마는 대통령이다.\",\n",
    "                \"시민들은 도시 속에 산다.\",\n",
    "                \"커피는 필요 없다.\",\n",
    "                \"일곱 명의 사망자가 발생했다.\"\n",
    "    ]\n",
    "\n",
    "    transformer = Transformer(n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len, dropout)\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        total_loss = 0\n",
    "\n",
    "        idx_list = list(range(0, enc_train.shape[0], batch_size))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm_notebook(idx_list)\n",
    "\n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            train_step(enc_train[idx:idx+batch_size],\n",
    "                        dec_train[idx:idx+batch_size],\n",
    "                        transformer,\n",
    "                        optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "        for example in examples:\n",
    "            translate(example, transformer, ko_tokenizer, en_tokenizer, plot_attention=False)\n",
    "        \n",
    "    plt.plot(total_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()    \n",
    "    \n",
    "    transformer.reset_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36422234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "n_layers = 2\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "pos_len = 200\n",
    "dropout = 0.1\n",
    "\n",
    "vocab_sizes = [20000, 30000, 50000]\n",
    "batch_sizes = [16,32,64]\n",
    "epochs = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92693c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10\n",
      "batch_size : 16\n",
      "vocab_size : 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1809/1188836346.py:27: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  t = tqdm_notebook(idx_list)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd847fde385479f8c13274eb1200075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3683 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_vocab_size = vocab_sizes\n",
    "tgt_vocab_size = vocab_sizes\n",
    "\n",
    "for epoch in epochs:\n",
    "    for batch_size in batch_sizes:\n",
    "        for vocab_size in vocab_sizes:\n",
    "            src_vocab_size = vocab_size\n",
    "            tgt_vocab_size = vocab_size\n",
    "\n",
    "            train_evaluate(epoch, enc_train, dec_train, optimizer, ko_tokenizer, en_tokenizer, src_vocab_size, tgt_vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2e50b",
   "metadata": {},
   "source": [
    "### 모델 밖의 조력자들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9fb46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad0d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:35:26.323895: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "1.5.3\n",
      "2.12.0\n",
      "3.7\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8633f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from konlpy.tag import Mecab,Okt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a39139",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777d01cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all files from ./data/ko.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# specify the zip file name\n",
    "zip_file_name = './data/ko.zip'\n",
    "\n",
    "# open the zip file in read mode\n",
    "with zipfile.ZipFile(zip_file_name, 'r') as zip:\n",
    "    # extract all files\n",
    "    zip.extractall()\n",
    "    print(f'Extracted all files from {zip_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef8c36",
   "metadata": {},
   "source": [
    "### Data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d587fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv('./data/ChatbotData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b245e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = data['Q']\n",
    "answer = data['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9bcbe",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb80dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_sentence_kor(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf269aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "\n",
    "def duplication_check(corpus1, corpus2):\n",
    "    #zipped = zip(corpus1, corpus2)\n",
    "    zipped = builtins.zip(corpus1, corpus2)\n",
    "    #unique_pairs = set(zipped)\n",
    "    unique_pairs = set(map(lambda x: (tuple(x[0]), tuple(x[1])), zipped))\n",
    "    \n",
    "    unique_pairs = list(unique_pairs)\n",
    "    corpus1, corpus2 = builtins.zip(*unique_pairs)\n",
    "    #corpus1, corpus2 = unique_pairs\n",
    "\n",
    "    return corpus1,corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d2b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sentence_lengths(series):\n",
    "    # Calculate the length of each sentence in the Series\n",
    "    lengths = series.str.len()\n",
    "\n",
    "    # Plot a histogram of sentence lengths\n",
    "    lengths.plot.hist()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f11f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sentence_lengths(question)\n",
    "plot_sentence_lengths(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7a34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc7100",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb16f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(corpus1, corpus2):\n",
    "    mecab_corpus_q = []\n",
    "    mecab_corpus_a = []\n",
    "    m = Mecab()\n",
    "    \n",
    "    for sentence in corpus1:\n",
    "        mecab_corpus_q.append(m.morphs(sentence))\n",
    "    \n",
    "    for sentence in corpus2:\n",
    "        mecab_corpus_a.append(m.morphs(sentence))\n",
    "    \n",
    "    #print(mecab_corpus_q,mecab_corpus_a )\n",
    "    #zipped = zip(mecab_corpus_q, mecab_corpus_a)\n",
    "    zipped = builtins.zip(mecab_corpus_q, mecab_corpus_a)\n",
    "    zipped_pd = pd.DataFrame(zipped, columns=['Q', 'A'])\n",
    "    #tokens_zipped_pd.head()\n",
    "    \n",
    "    mask = (zipped_pd['Q'].str.len() <= MAX_LENGTH) & (zipped_pd['A'].str.len() <= MAX_LENGTH)\n",
    "    result = zipped_pd[mask]\n",
    "    #print(result)\n",
    "    \n",
    "    corpus1 = result['Q']\n",
    "    corpus2 = result['A']\n",
    "    \n",
    "    return corpus1, corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e1406a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def build_corpus(corpus1, corpus2):\n",
    "    corpus1 = list(map(preprocess_sentence_kor,corpus1))  # 영문 대문자 소문자로 바꾸고 한글 정규 표현 적용\n",
    "    corpus2 = list(map(preprocess_sentence_kor, corpus2))\n",
    "    corpus1, corpus2 = sentence_tokenizer(corpus1,corpus2) # mecab.morph|s 형태소 분석기 사용, 문장길이 40으로 제한\n",
    "    que_corpus, ans_corpus =  duplication_check(corpus1, corpus2)  # 중복 체크\n",
    "    \n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0a5cf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11746, 11746)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus, ans_corpus =  build_corpus(question, answer)\n",
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a175c58",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ba3f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the pre-trained Korean(w) Word2Vec model\n",
    "model = Word2Vec.load('./data/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99dbe8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub_augmentation(tokenized_sentence, model):\n",
    "    n=3\n",
    "    similar_words=[]\n",
    "    \n",
    "    for r in range(3):\n",
    "        for word in tokenized_sentence:\n",
    "            similar_words.append(word)\n",
    "            if word in model.wv:\n",
    "                for i in range(n):\n",
    "                    similar_words.append(model.wv.most_similar(word, topn=n)[i][0])\n",
    "        break\n",
    "    return  similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63049b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus_aumented = []\n",
    "\n",
    "for tokenized_sentence in que_corpus:\n",
    "    augmented_sentence = lexical_sub_augmentation(tokenized_sentence, model)\n",
    "    if augmented_sentence is not None: \n",
    "        que_corpus_aumented.append(augmented_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7777954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11746, 11746, 4, 16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus), len(que_corpus_aumented),len(que_corpus[0]), len(que_corpus_aumented[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fac17f",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a86e6",
   "metadata": {},
   "source": [
    "### 타겟 데이터 전체에 <start> 토큰과 <end> 토큰을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbef6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_special_tokens(corpus):\n",
    "    print(len(corpus))\n",
    "    temp = []\n",
    "    for sentence in corpus:\n",
    "        temp.append( [\"<start>\"] + list(sentence) + [\"<end>\"])\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dae35fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11746\n"
     ]
    }
   ],
   "source": [
    "ans_corpus_special_token = adding_special_tokens(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f912b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11746, 11746)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus_aumented), len(ans_corpus_special_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb548bb",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a69da192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from typing import List\n",
    "\n",
    "def vectorize_corpus(corpus, model):\n",
    "    vectorized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        vectorized_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in model.wv:\n",
    "                vector = model.wv[word]\n",
    "                vectorized_sentence.append(vector)\n",
    "        vectorized_corpus.append(vectorized_sentence)\n",
    "    return vectorized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "406c2cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:39:57.674805: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "enc_train = vectorize_corpus(que_corpus_aumented, model)\n",
    "dec_train = vectorize_corpus(ans_corpus_special_token, model)\n",
    "\n",
    "max_input_length_enc = max(len(sublist) for sublist in enc_train)\n",
    "max_input_length_dec = max(len(sublist) for sublist in dec_train)\n",
    "max_input_length = max(max_input_length_enc, max_input_length_dec)\n",
    "\n",
    "enc_train_padding = pad_sequences(enc_train, maxlen=max_input_length, dtype='float32')\n",
    "dec_train_padding = pad_sequences(dec_train, maxlen=max_input_length, dtype='float32')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((enc_train_padding, dec_train_padding))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train_padding, dec_train_padding)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12ee4db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.43459892,  0.5294219 , -1.3228387 , ..., -0.5086767 ,\n",
       "         -0.7872419 , -0.44559422],\n",
       "        [ 0.7317492 ,  0.6601496 , -1.844165  , ..., -1.5735004 ,\n",
       "         -0.86070013,  0.19997895],\n",
       "        [ 0.1380372 , -0.05046546, -2.1609845 , ...,  0.00248639,\n",
       "         -0.83771986, -1.2966256 ]], dtype=float32),\n",
       " array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [-0.09237527, -0.5477274 ,  0.30446327, ...,  1.05425   ,\n",
       "          1.0875396 , -0.9107    ],\n",
       "        [ 1.6385531 ,  0.71334016, -1.1785264 , ..., -0.8688267 ,\n",
       "          1.0913216 , -0.7004538 ],\n",
       "        [ 0.746853  ,  0.23478179,  1.1706827 , ..., -0.6664024 ,\n",
       "          0.48546973, -0.23150781]], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train_padding[0], dec_train_padding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bad404f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_train_padding[0]), len(dec_train_padding[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ad8ee",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c579976",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bbce97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836c952",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1255a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39027be",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b670c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3a41b",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a7fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369bd08",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22e42d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4619b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76162b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0caea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46e4de",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a82e3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4625f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29694525",
   "metadata": {},
   "source": [
    "### Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights = word2vec.vectors\n",
    "pretrained_weights = model.wv.vectors\n",
    "\n",
    "# Define the embedding layer with pre-trained weights\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=pretrained_weights.shape[0],\n",
    "    output_dim=pretrained_weights.shape[1],\n",
    "    weights=[pretrained_weights]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2d986d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef93d86",
   "metadata": {},
   "source": [
    "### 모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "548673ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(enc_train_padding)\n",
    "VOCAB_SIZE\n",
    "\n",
    "d_model=512\n",
    "pos_len = max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2acd0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model= d_model,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    #pos_len=200,\n",
    "    pos_len=pos_len,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "#transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccbb70",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8abd2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a676d",
   "metadata": {},
   "source": [
    "### Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75be7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114270c9",
   "metadata": {},
   "source": [
    "### Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2623756",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81feefa1",
   "metadata": {},
   "source": [
    "### Attention 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "411ad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e70f6a",
   "metadata": {},
   "source": [
    "### 번역 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9442579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#그래프 한글표현 인스톨?\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94d3e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence_kor(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, \n",
    "                                                                 combined_mask, dec_padding_mask)\n",
    "\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de027df5",
   "metadata": {},
   "source": [
    "### 번역 생성 및 Attention 시각화 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42399935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42fc80",
   "metadata": {},
   "source": [
    "### Train Step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32607dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "544432f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    #tgt_in = tgt  # Decoder의 input\n",
    "    \n",
    "    print(\"tgt_in :\", tgt_in)\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7608a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a01c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                              | 0/184 [00:48<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 200) (64, 128, 200)\n",
      "tgt_in : Tensor(\"strided_slice:0\", shape=(64, 127, 200), dtype=float32)\n",
      "srcs (64, 128, 200)\n",
      "tgt.... (64, 127, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_14339/1041125753.py\", line 8, in train_step  *\n        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n    File \"/tmp/ipykernel_14339/4100722273.py\", line 15, in generate_masks  *\n        print(\"tgt\", tgt.shape.shape[1])\n\n    AttributeError: 'TensorShape' object has no attribute 'shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (enc_batch, dec_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(enc_batch\u001b[38;5;241m.\u001b[39mshape, dec_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m     batch_loss, enc_attns, dec_attns, dec_enc_attns \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdec_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[1;32m     19\u001b[0m     tqdm_bar\u001b[38;5;241m.\u001b[39mset_description_str(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%2d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filem0ipr14l.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(src, tgt, model, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_in :\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(tgt_in))\n\u001b[1;32m     12\u001b[0m gold \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(tgt)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 13\u001b[0m (enc_mask, dec_enc_mask, dec_mask) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_masks), (ag__\u001b[38;5;241m.\u001b[39mld(src), ag__\u001b[38;5;241m.\u001b[39mld(tgt_in)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     15\u001b[0m     (predictions, enc_attns, dec_attns, dec_enc_attns) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(src), ag__\u001b[38;5;241m.\u001b[39mld(tgt_in), ag__\u001b[38;5;241m.\u001b[39mld(enc_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_enc_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_mask)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileunkkryk7.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__generate_masks\u001b[0;34m(src, tgt)\u001b[0m\n\u001b[1;32m     12\u001b[0m dec_enc_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_padding_mask), (ag__\u001b[38;5;241m.\u001b[39mld(src),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt....\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(tgt)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(tgt)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     15\u001b[0m dec_lookahead_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_lookahead_mask), (ag__\u001b[38;5;241m.\u001b[39mld(tgt)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m dec_tgt_padding_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_padding_mask), (ag__\u001b[38;5;241m.\u001b[39mld(tgt),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_14339/1041125753.py\", line 8, in train_step  *\n        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n    File \"/tmp/ipykernel_14339/4100722273.py\", line 15, in generate_masks  *\n        print(\"tgt\", tgt.shape.shape[1])\n\n    AttributeError: 'TensorShape' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "# Q. 위의 코드를 활용하여 모델을 훈련시켜봅시다!\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        print(enc_batch.shape, dec_batch.shape)\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "        tqdm_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "493e45f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                              | 0/184 [03:25<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_14339/2376532002.py\", line 7, in train_step  *\n        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n    File \"/tmp/ipykernel_14339/875891818.py\", line 15, in generate_masks  *\n        dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n\n    ValueError: Dimensions must be equal, but are 200 and 127 for '{{node Maximum}} = Maximum[T=DT_FLOAT](strided_slice_4, sub)' with input shapes: [64,1,1,127,200], [127,127].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m tqdm_bar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mdataset_count)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (enc_batch, dec_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[1;32m     10\u001b[0m     batch_loss, enc_attns, dec_attns, dec_enc_attns \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdec_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[1;32m     18\u001b[0m     tqdm_bar\u001b[38;5;241m.\u001b[39mset_description_str(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%2d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filet1vq4cny.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(src, tgt, model, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m tgt_in \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(tgt)[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m gold \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(tgt)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 12\u001b[0m (enc_mask, dec_enc_mask, dec_mask) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_masks), (ag__\u001b[38;5;241m.\u001b[39mld(src), ag__\u001b[38;5;241m.\u001b[39mld(tgt_in)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     14\u001b[0m     (predictions, enc_attns, dec_attns, dec_enc_attns) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(src), ag__\u001b[38;5;241m.\u001b[39mld(tgt_in), ag__\u001b[38;5;241m.\u001b[39mld(enc_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_enc_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_mask)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filezgdhj44v.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__generate_masks\u001b[0;34m(src, tgt)\u001b[0m\n\u001b[1;32m     12\u001b[0m dec_lookahead_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_lookahead_mask), (ag__\u001b[38;5;241m.\u001b[39mld(tgt)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m dec_tgt_padding_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_padding_mask), (ag__\u001b[38;5;241m.\u001b[39mld(tgt),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 14\u001b[0m dec_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmaximum, (ag__\u001b[38;5;241m.\u001b[39mld(dec_tgt_padding_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_lookahead_mask)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_14339/2376532002.py\", line 7, in train_step  *\n        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n    File \"/tmp/ipykernel_14339/875891818.py\", line 15, in generate_masks  *\n        dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n\n    ValueError: Dimensions must be equal, but are 200 and 127 for '{{node Maximum}} = Maximum[T=DT_FLOAT](strided_slice_4, sub)' with input shapes: [64,1,1,127,200], [127,127].\n"
     ]
    }
   ],
   "source": [
    "# 훈련시키기\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a863050f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                              | 0/184 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 4.34598923e-01  5.29421926e-01 -1.32283866e+00 ... -5.08676708e-01\n",
      "   -7.87241876e-01 -4.45594221e-01]\n",
      "  [ 7.31749177e-01  6.60149574e-01 -1.84416497e+00 ... -1.57350039e+00\n",
      "   -8.60700130e-01  1.99978948e-01]\n",
      "  [ 1.38037205e-01 -5.04654609e-02 -2.16098452e+00 ...  2.48639099e-03\n",
      "   -8.37719858e-01 -1.29662561e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 2.73862690e-01 -3.30197275e-01 -1.42438158e-01 ... -8.08852494e-01\n",
      "    4.54800725e-02 -1.79088488e-02]\n",
      "  [ 3.91261411e+00 -1.11879528e+00 -3.30471039e-01 ... -2.01156473e+00\n",
      "    3.93370949e-02 -9.46636975e-01]\n",
      "  [-1.83959261e-01 -4.00336199e-02  1.82446510e-01 ... -6.30131781e-01\n",
      "    2.63034701e-01  1.55549552e-02]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 8.33489448e-02 -4.60788727e-01  9.23447683e-02 ...  3.99444461e-01\n",
      "   -7.43323639e-02  1.93190381e-01]\n",
      "  [ 1.14226568e+00  2.17419714e-01  5.04191399e-01 ...  8.39631379e-01\n",
      "    6.04118049e-01  2.96164244e-01]\n",
      "  [-7.35203147e-01 -7.21813619e-01  6.17995918e-01 ...  4.43578959e-01\n",
      "    7.87226975e-01  1.43901312e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [-4.34686184e-01  1.08154885e-01  2.87854195e-01 ...  2.87408888e-01\n",
      "    1.50410563e-01  3.49059910e-01]\n",
      "  [-3.84002298e-01  2.15236604e-01  6.90397382e-01 ...  3.72601151e-01\n",
      "    2.47705549e-01  1.85348555e-01]\n",
      "  [-1.23845112e+00 -3.05268347e-01  1.81017017e+00 ...  1.33307910e+00\n",
      "    4.08348531e-01  6.54594183e-01]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [-5.13763249e-01  1.86955035e+00 -6.03905499e-01 ... -3.96715665e+00\n",
      "    2.43020201e+00  3.46640229e-01]\n",
      "  [ 1.26600862e+00  1.07710457e+00 -1.34864271e+00 ... -2.18606186e+00\n",
      "    1.26154351e+00  6.76004946e-01]\n",
      "  [-1.13995271e-02  6.07037783e-01 -1.24573016e+00 ... -3.36942244e+00\n",
      "    2.82668114e+00  2.30284229e-01]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 3.86609882e-01  1.21385366e-01  1.01117587e+00 ...  6.92509711e-01\n",
      "    4.20470893e-01  3.14679384e-01]\n",
      "  [ 2.94184238e-01  2.64531344e-01  7.88187981e-01 ... -1.13023333e-01\n",
      "    5.46095252e-01  4.48496103e-01]\n",
      "  [-4.06401195e-02  1.49366423e-01  1.97785020e-01 ...  4.15276550e-02\n",
      "   -4.79136109e-02  4.09995109e-01]]], shape=(64, 128, 200), dtype=float32) tf.Tensor(\n",
      "[[[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-0.09237527 -0.5477274   0.30446327 ...  1.05425     1.0875396\n",
      "   -0.9107    ]\n",
      "  [ 1.6385531   0.71334016 -1.1785264  ... -0.8688267   1.0913216\n",
      "   -0.7004538 ]\n",
      "  [ 0.746853    0.23478179  1.1706827  ... -0.6664024   0.48546973\n",
      "   -0.23150781]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.4200385  -0.27882016  1.00197    ...  0.14929247  0.78602254\n",
      "   -0.05218609]\n",
      "  [-2.2476425  -0.45302594 -0.25164333 ...  2.2434034   2.0864313\n",
      "   -0.529551  ]\n",
      "  [ 0.746853    0.23478179  1.1706827  ... -0.6664024   0.48546973\n",
      "   -0.23150781]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.34129003  2.7620587  -3.1343217  ... -1.0930233   0.90010613\n",
      "   -1.2246209 ]\n",
      "  [ 1.6385531   0.71334016 -1.1785264  ... -0.8688267   1.0913216\n",
      "   -0.7004538 ]\n",
      "  [ 0.746853    0.23478179  1.1706827  ... -0.6664024   0.48546973\n",
      "   -0.23150781]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 1.3817106   1.0779619   1.9429834  ...  2.1113126  -2.5899303\n",
      "    1.2127101 ]\n",
      "  [ 1.3818619   0.86151385  0.9055838  ...  0.7134168   0.97391176\n",
      "    0.07371458]\n",
      "  [ 0.746853    0.23478179  1.1706827  ... -0.6664024   0.48546973\n",
      "   -0.23150781]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-1.3740246  -1.7353463   3.3915305  ... -1.4845604  -1.5236791\n",
      "   -1.5451258 ]\n",
      "  [-0.09237527 -0.5477274   0.30446327 ...  1.05425     1.0875396\n",
      "   -0.9107    ]\n",
      "  [ 0.746853    0.23478179  1.1706827  ... -0.6664024   0.48546973\n",
      "   -0.23150781]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 3.499531   -0.6843342   0.40073013 ...  3.845454    0.5002854\n",
      "    2.5895011 ]\n",
      "  [ 0.29507878 -0.5158134   1.2761102  ...  0.51102996 -0.04903798\n",
      "   -1.0467119 ]\n",
      "  [ 0.746853    0.23478179  1.1706827  ... -0.6664024   0.48546973\n",
      "   -0.23150781]]], shape=(64, 128, 200), dtype=float32) <__main__.Transformer object at 0x7f9705ee98b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_14339/2376532002.py\", line 7, in train_step  *\n        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n    File \"/tmp/ipykernel_14339/875891818.py\", line 15, in generate_masks  *\n        dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n\n    ValueError: Dimensions must be equal, but are 200 and 127 for '{{node Maximum}} = Maximum[T=DT_FLOAT](strided_slice_4, sub)' with input shapes: [64,1,1,127,200], [127,127].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (src, tgt)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(src, tgt, transformer)\n\u001b[0;32m---> 14\u001b[0m     batch_loss, enc_attns, dec_attns, dec_enc_attns \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[1;32m     17\u001b[0m     tqdm_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filet1vq4cny.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(src, tgt, model, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m tgt_in \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(tgt)[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m gold \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(tgt)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 12\u001b[0m (enc_mask, dec_enc_mask, dec_mask) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_masks), (ag__\u001b[38;5;241m.\u001b[39mld(src), ag__\u001b[38;5;241m.\u001b[39mld(tgt_in)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     14\u001b[0m     (predictions, enc_attns, dec_attns, dec_enc_attns) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(src), ag__\u001b[38;5;241m.\u001b[39mld(tgt_in), ag__\u001b[38;5;241m.\u001b[39mld(enc_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_enc_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_mask)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filezgdhj44v.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__generate_masks\u001b[0;34m(src, tgt)\u001b[0m\n\u001b[1;32m     12\u001b[0m dec_lookahead_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_lookahead_mask), (ag__\u001b[38;5;241m.\u001b[39mld(tgt)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m dec_tgt_padding_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generate_padding_mask), (ag__\u001b[38;5;241m.\u001b[39mld(tgt),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 14\u001b[0m dec_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmaximum, (ag__\u001b[38;5;241m.\u001b[39mld(dec_tgt_padding_mask), ag__\u001b[38;5;241m.\u001b[39mld(dec_lookahead_mask)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_14339/2376532002.py\", line 7, in train_step  *\n        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n    File \"/tmp/ipykernel_14339/875891818.py\", line 15, in generate_masks  *\n        dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n\n    ValueError: Dimensions must be equal, but are 200 and 127 for '{{node Maximum}} = Maximum[T=DT_FLOAT](strided_slice_4, sub)' with input shapes: [64,1,1,127,200], [127,127].\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    \n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        print(src, tgt, transformer)\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_description(f'Epoch {epoch + 1} Loss: {total_loss / (batch + 1):.4f}')\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} Loss: {total_loss / dataset_count:.6f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

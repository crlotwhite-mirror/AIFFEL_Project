{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad0d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 15:50:28.306281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "1.5.3\n",
      "2.12.0\n",
      "3.7\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8633f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from konlpy.tag import Mecab,Okt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a39139",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777d01cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/ko.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m zip_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/ko.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# open the zip file in read mode\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mzip\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# extract all files\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mzip\u001b[39m\u001b[38;5;241m.\u001b[39mextractall()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted all files from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.9/zipfile.py:1239\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1241\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/ko.zip'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# specify the zip file name\n",
    "zip_file_name = './data/ko.zip'\n",
    "\n",
    "# open the zip file in read mode\n",
    "with zipfile.ZipFile(zip_file_name, 'r') as zip:\n",
    "    # extract all files\n",
    "    zip.extractall()\n",
    "    print(f'Extracted all files from {zip_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef8c36",
   "metadata": {},
   "source": [
    "### Data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d587fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv('./data/ChatbotData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b245e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = data['Q']\n",
    "answer = data['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9bcbe",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_sentence_kor(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf269aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "\n",
    "def duplication_check(corpus1, corpus2):\n",
    "    #zipped = zip(corpus1, corpus2)\n",
    "    zipped = builtins.zip(corpus1, corpus2)\n",
    "    #unique_pairs = set(zipped)\n",
    "    unique_pairs = set(map(lambda x: (tuple(x[0]), tuple(x[1])), zipped))\n",
    "    \n",
    "    unique_pairs = list(unique_pairs)\n",
    "    corpus1, corpus2 = builtins.zip(*unique_pairs)\n",
    "    #corpus1, corpus2 = unique_pairs\n",
    "\n",
    "    return corpus1,corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sentence_lengths(series):\n",
    "    # Calculate the length of each sentence in the Series\n",
    "    lengths = series.str.len()\n",
    "\n",
    "    # Plot a histogram of sentence lengths\n",
    "    lengths.plot.hist()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f11f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentence_lengths(question)\n",
    "plot_sentence_lengths(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc7100",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb16f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(corpus1, corpus2):\n",
    "    mecab_corpus_q = []\n",
    "    mecab_corpus_a = []\n",
    "    m = Mecab()\n",
    "    \n",
    "    for sentence in corpus1:\n",
    "        mecab_corpus_q.append(m.morphs(sentence))\n",
    "    \n",
    "    for sentence in corpus2:\n",
    "        mecab_corpus_a.append(m.morphs(sentence))\n",
    "    \n",
    "    #print(mecab_corpus_q,mecab_corpus_a )\n",
    "    #zipped = zip(mecab_corpus_q, mecab_corpus_a)\n",
    "    zipped = builtins.zip(mecab_corpus_q, mecab_corpus_a)\n",
    "    zipped_pd = pd.DataFrame(zipped, columns=['Q', 'A'])\n",
    "    #tokens_zipped_pd.head()\n",
    "    \n",
    "    mask = (zipped_pd['Q'].str.len() <= MAX_LENGTH) & (zipped_pd['A'].str.len() <= MAX_LENGTH)\n",
    "    result = zipped_pd[mask]\n",
    "    #print(result)\n",
    "    \n",
    "    corpus1 = result['Q']\n",
    "    corpus2 = result['A']\n",
    "    \n",
    "    return corpus1, corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1406a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def build_corpus(corpus1, corpus2):\n",
    "    corpus1 = list(map(preprocess_sentence_kor,corpus1))  # 영문 대문자 소문자로 바꾸고 한글 정규 표현 적용\n",
    "    corpus2 = list(map(preprocess_sentence_kor, corpus2))\n",
    "    corpus1, corpus2 = sentence_tokenizer(corpus1,corpus2) # mecab.morph|s 형태소 분석기 사용, 문장길이 40으로 제한\n",
    "    que_corpus, ans_corpus =  duplication_check(corpus1, corpus2)  # 중복 체크\n",
    "    \n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus =  build_corpus(question, answer)\n",
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a175c58",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the pre-trained Korean(w) Word2Vec model\n",
    "model = Word2Vec.load('./data/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbe8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub_augmentation(tokenized_sentence, model):\n",
    "    n=3\n",
    "    similar_words=[]\n",
    "    \n",
    "    for r in range(3):\n",
    "        for word in tokenized_sentence:\n",
    "            similar_words.append(word)\n",
    "            if word in model.wv:\n",
    "                for i in range(n):\n",
    "                    similar_words.append(model.wv.most_similar(word, topn=n)[i][0])\n",
    "        break\n",
    "    return  similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63049b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus_aumented = []\n",
    "\n",
    "for tokenized_sentence in que_corpus:\n",
    "    augmented_sentence = lexical_sub_augmentation(tokenized_sentence, model)\n",
    "    if augmented_sentence is not None: \n",
    "        que_corpus_aumented.append(augmented_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7777954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(que_corpus), len(que_corpus_aumented),len(que_corpus[0]), len(que_corpus_aumented[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7befbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus[0],que_corpus_aumented[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fac17f",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a86e6",
   "metadata": {},
   "source": [
    "### 타겟 데이터 전체에 <start> 토큰과 <end> 토큰을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_special_tokens(corpus):\n",
    "    print(len(corpus))\n",
    "    temp = []\n",
    "    for sentence in corpus:\n",
    "        temp.append( [\"<start>\"] + list(sentence) + [\"<end>\"])\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae35fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ans_corpus_special_token = adding_special_tokens(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f912b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(que_corpus_aumented), len(ans_corpus_special_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb548bb",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69da192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from typing import List\n",
    "\n",
    "def vectorize_corpus(corpus, model):\n",
    "    vectorized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        vectorized_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in model.wv:\n",
    "                vector = model.wv[word]\n",
    "                vectorized_sentence.append(vector)\n",
    "        vectorized_corpus.append(vectorized_sentence)\n",
    "    return vectorized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = vectorize_corpus(que_corpus_aumented, model)\n",
    "dec_train = vectorize_corpus(ans_corpus_special_token, model)\n",
    "\n",
    "max_input_length_enc = max(len(sublist) for sublist in enc_train)\n",
    "max_input_length_dec = max(len(sublist) for sublist in dec_train)\n",
    "max_input_length = max(max_input_length_enc, max_input_length_dec)\n",
    "\n",
    "enc_train_padding = pad_sequences(enc_train, maxlen=max_input_length, dtype='float32')\n",
    "dec_train_padding = pad_sequences(dec_train, maxlen=max_input_length, dtype='float32')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((enc_train_padding, dec_train_padding))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train_padding, dec_train_padding)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_padding[0], dec_train_padding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enc_train_padding[0]), len(dec_train_padding[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ad8ee",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c579976",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbce97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836c952",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39027be",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3a41b",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369bd08",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e42d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4619b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76162b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0caea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46e4de",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29694525",
   "metadata": {},
   "source": [
    "### Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_weights = word2vec.vectors\n",
    "pretrained_weights = model.wv.vectors\n",
    "\n",
    "# Define the embedding layer with pre-trained weights\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=pretrained_weights.shape[0],\n",
    "    output_dim=pretrained_weights.shape[1],\n",
    "    weights=[pretrained_weights]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d986d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef93d86",
   "metadata": {},
   "source": [
    "### 모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548673ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(enc_train_padding)\n",
    "VOCAB_SIZE\n",
    "\n",
    "d_model=512\n",
    "pos_len = max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model= d_model,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    #pos_len=200,\n",
    "    pos_len=pos_len,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "#transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccbb70",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a676d",
   "metadata": {},
   "source": [
    "### Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114270c9",
   "metadata": {},
   "source": [
    "### Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2623756",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81feefa1",
   "metadata": {},
   "source": [
    "### Attention 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e70f6a",
   "metadata": {},
   "source": [
    "### 번역 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#그래프 한글표현 인스톨?\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence_kor(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, \n",
    "                                                                 combined_mask, dec_padding_mask)\n",
    "\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de027df5",
   "metadata": {},
   "source": [
    "### 번역 생성 및 Attention 시각화 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42399935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42fc80",
   "metadata": {},
   "source": [
    "### Train Step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32607dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544432f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    #tgt_in = tgt  # Decoder의 input\n",
    "    \n",
    "    print(\"tgt_in :\", tgt_in)\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 위의 코드를 활용하여 모델을 훈련시켜봅시다!\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        print(enc_batch.shape, dec_batch.shape)\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "        tqdm_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e45f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 훈련시키기\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863050f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    \n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        print(src, tgt, transformer)\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_description(f'Epoch {epoch + 1} Loss: {total_loss / (batch + 1):.4f}')\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} Loss: {total_loss / dataset_count:.6f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
